---
title: "Prediction of Activity by Performance Measurement"
author: "Matthew Gast"
date: "October 2015"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
  pdf_document:
    fig_height: 3
---

## Background

Quantified self devices collect data on personal activity.  One
limitation of these devices is that users regularly measure the amount
of an activity, but not its quality.  This project uses accelerometer
data from a series of activities done both correctly and incorrectly.

Six subjects in the project were asked to lift barbells in one of five
different ways:

+ Class A: exactly according to the specification
+ Class B: throwing the elbows to the front
+ Class C: lifting the dumbbell only halfway
+ Class D: lowering the dumbbell only halfway
+ Class E: throwing the hips to the front

More information is available from the website here:
http://groupware.les.inf.puc-rio.br/har (see the section on the Weight
Lifting Exercise Dataset).

## Goal

The purpose of this project is to use data to predict the manner in
which the weight lift was performed.  The outcome is the activity
type, stored in the "classe" variable in the training set.

## Load the data

```{r setup_environment, results='hide', message=FALSE, warning=FALSE}
source("prediction.R")
loadPackages()
setConstants()
registerDoParallel(2)
```

Begin by downloading the data and reading it into data structures.
The read functions defined for this project automatically convert
Excel division-by-zero (#DIV/O!") and unavailable values into R's NA
value.

```{r download_data}
download.pmlfiles(proj.dir)
training.raw <- read.pmlfile("pml-training.csv",proj.dir)
testing.raw <- read.pmlfile("pml-testing.csv",proj.dir)
```

## Clean Data

To clean the data, remove columns with NA values and the metadata
columns.  Metadata columns contain timestamps and bucket numbers that
should not be used in prediction models.  Finally, remove any columns
that have near-zero variance because they lack predictive power.

```{r clean_data}
training.data <- remove.na.cols(training.raw)
training.data <- remove.metadata.cols(training.data)
testing.data <- remove.na.cols(testing.raw)
testing.data <- remove.metadata.cols(testing.data)
nsv.cols <- nearZeroVar (training.data)
if (length(nsv.cols) > 0) {
    training.data <- training.data[-nsv.cols]
}
```

## Cross-Validation

This paper uses 5-fold cross validation, set up with the
`trainControl()` method.  By passing the resulting data structure to
the `train()` function when assessing models, R will automatically
perform resampling and cross-validation.

```{r cv_setup}
CV5fold <- trainControl(method="cv",5)
```

As part of model validation, split the data into a training component
and a test component, where the split is controlled by a global
variable.  This paper uses 60% of the data for training, and 40% for
testing, but can be changed easily in the `setConstants()` function.

```{r splitdata}
in.train <- createDataPartition(training.data$classe, p=train.pct, list=FALSE)
train.set <- training.data[in.train,]
test.set <- training.data[-in.train,]
```

## Model building

The dependent variable in the model is a factor, so re-cast the output
of the prediction function as a factor.  In the training data, remove
the dependent variable, which is stored in the last column.

```{r model_setup}
y <- as.factor(train.set$classe)
x <- train.set[-ncol(train.set)]
```

### Random Forest model

The first model assessed is a random forest model, which generally has
good predictive power but can be quite slow.  To improve speed, this
paper was set up to run on a 2-core machine.  Random forests can be
parallelized by using the `parRF` method in the `train()` function.
To include cross-validation, the `trControl` option is set to use
cross-validation.  After building the model, get its accuracy based on
its ability to predict the value for the test data.

```{r rf_model}
model.rf <- train(y~.,data=x,method="parRF",trControl=CV5fold)
rf.test.pred <- predict (model.rf, test.set)
cm.rf <- confusionMatrix(rf.test.pred,test.set$classe)
```

The accuracy for the model is `r cm.rf$overall[1]`, and the estimated
OOB error is `r 1-cm.rf$overall[1]`

### Decision tree model

For the second model type, set up a classification tree.  After
building the model, obtain its accuracy on the test data.

```{r dt_model}
model.dt <- rpart(y ~ ., data=x, method="class")
dt.test.pred <- predict(model.dt, test.set, type="class")
cm.dt <- confusionMatrix(dt.test.pred, test.set$classe)
```

Accuracy is `r cm.dt$overall[1]`, so the estimated
OOB error is `r 1-cm.dt$overall[1]`

### Boosted model

Finally, consider a generic boosted model, again with a 5-fold cross-validation.

```{r boost_model}
model.gbm <- train(y~.,data=x,method="gbm",trControl=CV5fold)
gbm.test.pred <-predict(model.gbm, test.set)
cm.gbm <- confusionMatrix(gbm.test.pred, test.set$class)
```

Accuracy is `r cm.gbm$overall[1]`, so the estimated
OOB error is `r 1-cm.gbm$overall[1]`

### Model choice

The random forest and boosted models both have very high accuracy.  To
further compare them, consider the relative importance of the top 20
varables in each model.

```{r graphs}
varimp.rf.g <- plot(varImp(model.rf,scale=FALSE))
varimp.gbm.g <- plot(varImp(model.gbm,scale=FALSE))
grid.arrange(varimp.rf.g,varimp.gbm.g,ncol=2)
```

Both models use the idential top three variables in predictions, and
have similar weights for most variables.  Without strong theoretical
reasons to choose one model over the other, we select the random
forest model because it has slightly better accuracy than the boosted
model.

## Test Data Prediction

As a final step, take the test data and make predictions based on the
random forest model.  Those predictions are then written into the
current working directory and uploaded for the second part of the
project.

```{r rf_test_prediction_write} 
test.pred <- predict(model.rf, testing.raw)
pml_write_files(test.pred)
```

